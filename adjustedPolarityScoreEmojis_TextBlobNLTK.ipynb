{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/kevinvo/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/kevinvo/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/kevinvo/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as tkr\n",
    "import seaborn as sns\n",
    "import scipy as stats\n",
    "\n",
    "import nltk # sentiment library\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('vader_lexicon') # download vader lexicon\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provides relative paths for csv as referenced from a folder\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('data'): #path to folder\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/kevinvo/CUBigDataClass'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "source": [
    "**README**\n",
    "- emot: library used to process emoji and emoticons\n",
    "\n",
    "- first see polarity adjusted score with TextBlob \n",
    "\n",
    "- then compare adjusted polarity scores using NLTK's sentiment analyzer afterwards"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**processing emojis via emot library:**\n",
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: emot in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Via pip:\n",
    "# $ pip install emot --upgrade\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install emot --upgrade\n",
    "\n",
    "# 2. From master branch: \n",
    "# $ git clone https://github.com/NeelShah18/emot.git\n",
    "# $ cd emot\n",
    "# $ python setup.py install\n",
    "\n",
    "import emot\n",
    "\n",
    "def clean_mean(val):\n",
    "    return val.replace('_', ' ').replace('-', ' ').replace(':', ' ')\n",
    "\n",
    "def convert_emojicon(text, isPrint=True):\n",
    "    for emoti in emot.emo_unicode.EMOTICONS:\n",
    "        if emoti in text:\n",
    "            text = text.replace(emoti, clean_mean(emot.emo_unicode.EMOTICONS.get(emoti, '')))\n",
    "            if isPrint==True:\n",
    "                print(emoti)\n",
    "            \n",
    "    for emoti in emot.emo_unicode.UNICODE_EMO:\n",
    "        if emoti in text:\n",
    "            text = text.replace(emoti, clean_mean(emot.emo_unicode.UNICODE_EMO.get(emoti, '')))\n",
    "            if isPrint==True:\n",
    "                print(emoti)\n",
    "            \n",
    "    for emoti in emot.emo_unicode.EMOTICONS_EMO:\n",
    "        if emoti in text:\n",
    "            text = text.replace(emoti, clean_mean(emot.emo_unicode.EMOTICONS_EMO.get(emoti, '')))\n",
    "            if isPrint==True:\n",
    "                print(emoti)\n",
    "    return text"
   ]
  },
  {
   "source": [
    "**via TextBlob Sentiment Analyser:**\n",
    "----\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def compareAdjustedScores(text):\n",
    "    adjusted_text = convert_emojicon(text, isPrint==False)\n",
    "    print(\"pre-adjustments: {}; post-adjustments: {}\".format(TextBlob(text).sentiment.polarity, TextBlob(adjusted_text).sentiment.polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokenize(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenzr = RegexpTokenizer('\\s+', gaps = True)\n",
    "    \n",
    "    tokenized_text = tokenzr.tokenize(text.lower())\n",
    "    words = [lemmatizer.lemmatize(w) for w in tokenized_text if w not in stop_words]\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in words])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üòû\npre-adjustments: 0.0; post-adjustments: -0.75\n\n---\n\nüôÅ\npre-adjustments: 0.0; post-adjustments: -0.16666666666666666\n\n---\n\nüòÄ\npre-adjustments: 0.0; post-adjustments: 0.0\n\n---\n\nüôÇ\npre-adjustments: 0.0; post-adjustments: -0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "isPrint = True\n",
    "#\n",
    "sample_sad = \"üòû\"\n",
    "sample_sad=stem_tokenize(sample_sad)\n",
    "convert_emojicon(sample_sad, isPrint==True)\n",
    "compareAdjustedScores(sample_sad)\n",
    "print(\"\\n---\\n\")\n",
    "#\n",
    "sample_sad2 = \"üôÅ\"\n",
    "sample_sad2=stem_tokenize(sample_sad2)\n",
    "convert_emojicon(sample_sad2, isPrint==True)\n",
    "compareAdjustedScores(sample_sad2)\n",
    "print(\"\\n---\\n\")\n",
    "#\n",
    "sample_happy = \"üòÄ\"\n",
    "sample_happy=stem_tokenize(sample_happy)\n",
    "convert_emojicon(sample_happy, isPrint==True)\n",
    "compareAdjustedScores(sample_happy)\n",
    "print(\"\\n---\\n\")\n",
    "#\n",
    "sample_happy2= \"üôÇ\"\n",
    "sample_happy2=stem_tokenize(sample_happy2)\n",
    "convert_emojicon(sample_happy2, isPrint==True)\n",
    "compareAdjustedScores(sample_happy2)"
   ]
  },
  {
   "source": [
    "**via NLTK Sentiment Analyser:**\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nCase 1a:  disappointed face\n pre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n post-emoji processing: {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n\nbefore: 0.0; after: -0.4767\nüòû\n disappointed face \n\nCase 1b:  slightly frowning face \n pre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n post-emoji processing: {'neg': 0.513, 'neu': 0.487, 'pos': 0.0, 'compound': -0.2748}\n\nbefore: 0.0; after: -0.2748\nüôÅ\n slightly frowning face \n\nCase 2a: grinning face \npre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\npost-emoji processing: {'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}\n\nbefore: 0.0; after: 0.3612\nüòÄ\n grinning face \n\nCase 2b: slightly smiling face \n pre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n post-emoji processing: {'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.4033}\n\nbefore: 0.0; after: 0.4033\nüôÇ\n slightly smiling face \n"
     ]
    }
   ],
   "source": [
    "sia = SIA()\n",
    "# initialize object first, as SIA (from NLTK) is an object class\n",
    "\n",
    "###\n",
    "sample_text_sad1a = \"üòû\"\n",
    "score_sad1a, processedScore_sad1a = sia.polarity_scores(sample_text_sad1a), sia.polarity_scores(convert_emojicon(sample_text_sad1a, isPrint==False))\n",
    "print(\"\\nCase 1a:  disappointed face\\n pre-emoji processing: {}\\n post-emoji processing: {}\\n\".format(score_sad1a, processedScore_sad1a))\n",
    "print(\"before: {}; after: {}\".format(score_sad1a['compound'], processedScore_sad1a['compound'],'\\n'))\n",
    "\n",
    "print(convert_emojicon(sample_text_sad1a, isPrint==True))\n",
    "####\n",
    "\n",
    "sample_text_sad1b = \"üôÅ\"\n",
    "score_sad1b, processedScore_sad1b = sia.polarity_scores(sample_text_sad1b), sia.polarity_scores(convert_emojicon(sample_text_sad1b, isPrint==False))\n",
    "print(\"\\nCase 1b:  slightly frowning face \\n pre-emoji processing: {}\\n post-emoji processing: {}\\n\".format(score_sad1b, processedScore_sad1b))\n",
    "print(\"before: {}; after: {}\".format(score_sad1b['compound'], processedScore_sad1b['compound'],'\\n'))\n",
    "\n",
    "print(convert_emojicon(sample_text_sad1b, isPrint==True))\n",
    "###\n",
    "\n",
    "sample_text_happy2a = \"üòÄ\"\n",
    "print(\"\\nCase 2a: grinning face \\npre-emoji processing: {}\\npost-emoji processing: {}\\n\".format(sia.polarity_scores(sample_text_happy2a), sia.polarity_scores(convert_emojicon(sample_text_happy2a, isPrint==False))))\n",
    "print(\"before: {}; after: {}\".format(sia.polarity_scores(sample_text_happy2a)['compound'], sia.polarity_scores(convert_emojicon(sample_text_happy2a, isPrint==False))['compound'],'\\n'))\n",
    "\n",
    "print(convert_emojicon(sample_text_happy2a, isPrint==True))\n",
    "###\n",
    "\n",
    "sample_text_happy2b = \"üôÇ\"\n",
    "score_happy2b, processedScore_happy2b = sia.polarity_scores(sample_text_happy2b), sia.polarity_scores(convert_emojicon(sample_text_happy2b, isPrint==False))\n",
    "print(\"\\nCase 2b: slightly smiling face \\n pre-emoji processing: {}\\n post-emoji processing: {}\\n\".format(score_happy2b, processedScore_happy2b))\n",
    "print(\"before: {}; after: {}\".format(score_happy2b['compound'], processedScore_happy2b['compound'],'\\n'))\n",
    "\n",
    "print(convert_emojicon(sample_text_happy2b, isPrint==True))"
   ]
  },
  {
   "source": [
    "----\n",
    "- adjusting score, without tokenization\n",
    "\n",
    "- adjusting score, then tokenization \n",
    "\n",
    "(to eval how tokenization influences adjusted polarity scores for emojis)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Case 3: no tokenization, grinning face \npre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\npost-emoji processing: {'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}\n\nadjust score without tokenization:  0.0 0.3612 \n\nCase 3: +tokenization, grinning face \npre-emoji processing: {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\npost-emoji processing: {'neg': 0.0, 'neu': 0.244, 'pos': 0.756, 'compound': 0.4767}\n\nadjust score WITH tokenization:  0.0 0.4767 \n\n"
     ]
    }
   ],
   "source": [
    "### tokenize than convert\n",
    "toBe_Adjusted = \"üòÄ\"\n",
    "sample_adjusted=convert_emojicon(toBe_Adjusted, isPrint==False)\n",
    "print(\"Case 3: no tokenization, grinning face \\npre-emoji processing: {}\\npost-emoji processing: {}\\n\".format(sia.polarity_scores(toBe_Adjusted), sia.polarity_scores(sample_adjusted)))\n",
    "\n",
    "print(\"adjust score without tokenization: \",sia.polarity_scores(toBe_Adjusted)['compound'], sia.polarity_scores(sample_adjusted)['compound'],'\\n')\n",
    "\n",
    "\n",
    "### tokenize after converting\n",
    "toBe_Adjusted_Tokenized = \"üòÄ\"\n",
    "sample_adjusted=convert_emojicon(toBe_Adjusted_Tokenized, isPrint==False)\n",
    "sample_adjustedtokenized=stem_tokenize(sample_adjusted)\n",
    "convert_emojicon(toBe_Adjusted_Tokenized, isPrint==False)\n",
    "print(\"Case 3: +tokenization, grinning face \\npre-emoji processing: {}\\npost-emoji processing: {}\\n\".format(sia.polarity_scores(toBe_Adjusted_Tokenized), sia.polarity_scores(sample_adjustedtokenized)))\n",
    "\n",
    "print(\"adjust score WITH tokenization: \", sia.polarity_scores(toBe_Adjusted_Tokenized)['compound'], sia.polarity_scores(sample_adjustedtokenized)['compound'],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'grin face'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "sample_adjustedtokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' grinning face '"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sample_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'üòÄ'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "toBe_Adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python390jvsc74a57bd07812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}